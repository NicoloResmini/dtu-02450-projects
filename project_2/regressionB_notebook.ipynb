{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import and standardize datas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.pyplot import (figure, title, boxplot, xticks, subplot, hist,\n",
    "                               xlabel, ylim, yticks, show, savefig)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('hour.csv')\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "# Removing useless attributes\n",
    "df = df.drop('dteday', axis=1)\n",
    "df = df.drop('instant', axis=1)\n",
    "df = df.drop('yr', axis=1)\n",
    "\n",
    "\n",
    "# Applying sqrt to \"cnt\" (to make it a continuous variable)\n",
    "df['cnt'] = np.sqrt(df['cnt'])\n",
    "\n",
    "# Removing deprecated attributes after the sqrt transformation (cnt = casual + registered)\n",
    "df = df.drop('casual', axis=1)\n",
    "df = df.drop('registered', axis=1)\n",
    "df = df.drop('atemp', axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_full = df.values\n",
    "scaler = StandardScaler()\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "df = pd.DataFrame(X_full_scaled, columns=df.columns)\n",
    "\n",
    "X = df.drop(columns=['cnt']).values\n",
    "attributeNames = df.columns.drop(['cnt']).tolist()\n",
    "N, M = X.shape\n",
    "y = df['cnt'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define variables for double cross validation\n",
    "\n",
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot\n",
    "import numpy as np\n",
    "from toolbox_02450 import train_neural_net, draw_neural_net\n",
    "import torch\n",
    "from toolbox_02450 import rlr_validate\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K1 = 10\n",
    "K2 = 10\n",
    "CV1 = model_selection.KFold(n_splits=K1,shuffle=True)\n",
    "size_val = np.empty(K2)\n",
    "size_par = np.empty(K1)\n",
    "size_test = np.empty(K1)\n",
    "  \n",
    "#ANN\n",
    "h = 0\n",
    "h_values = [1, 10, 50, 75, 100]\n",
    "h_number = len(h_values)\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 5000\n",
    "loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "Eval_ANN = np.empty((K2,h_number))\n",
    "Egen_ANN_temp = np.empty((h_number))\n",
    "Etest_ANN = np.empty(K1)\n",
    "optimal_ANN = np.empty(K1) \n",
    "\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.squeeze(input)\n",
    "\n",
    "#baseline \n",
    "Error_val_nofeatures = np.empty((K2,1))\n",
    "Etest_nofeatures = np.empty((K1,1))\n",
    "\n",
    "#Linear regression\n",
    "lambdas = np.power(10.,range(-1,3))\n",
    "opt_lambda = np.empty((K1,1)) \n",
    "mu = np.empty((K1, M-1))\n",
    "sigma = np.empty((K1, M-1))\n",
    "w_rlr = np.empty((M,K1))\n",
    "w = np.empty((M,K2,len(lambdas)))\n",
    "Error_test_rlr = np.empty((K1,1))\n",
    "Eval_rlr = np.empty((K2,len(lambdas))) \n",
    "Egen_rlr_temp = np.empty((len(lambdas)))\n",
    "\n",
    " # %% Double crosss validation\n",
    " \n",
    "k_1 = 0\n",
    "for par_index, test_index in CV1.split(X):\n",
    "    \n",
    "    \n",
    "    X_par = X[par_index,:]\n",
    "    y_par = y[par_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]      \n",
    "    size_par[k_1] = len(par_index)\n",
    "    size_test[k_1] = len(test_index)\n",
    "    X_par_rlr = X[par_index,:]\n",
    "    X_test_rlr = X[test_index,:]\n",
    "    \n",
    "    X_test_ANN = torch.from_numpy(X_test).float()\n",
    "    y_test_ANN = torch.from_numpy(y_test).float()\n",
    "    X_par_ANN = torch.from_numpy(X_par).float()\n",
    "    y_par_ANN = torch.from_numpy(y_par).float()\n",
    "    \n",
    "    CV2 = model_selection.KFold(n_splits=K2,shuffle=True)\n",
    "    \n",
    "    k_2 = 0\n",
    "    for train_index, val_index in CV2.split(X_par):\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train = X[train_index,:]\n",
    "        y_train = y[train_index]\n",
    "        X_val = X[val_index,:]\n",
    "        y_val = y[val_index]      \n",
    "        size_val[k_2] = len(val_index) \n",
    "        X_train_rlr = X[train_index,:]\n",
    "        X_val_rlr = X[val_index,:]\n",
    "        \n",
    "        X_train_ANN = torch.from_numpy(X_train).float()\n",
    "        y_train_ANN = torch.from_numpy(y_train).float()\n",
    "        X_val_ANN = torch.from_numpy(X_val).float()\n",
    "        y_val_ANN = torch.from_numpy(y_val).float()\n",
    "    \n",
    "    \n",
    "        #ANN\n",
    "        \n",
    "        \n",
    "        h_index = 0\n",
    "        for h in h_values:\n",
    "            # Define the model\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, h), #M features to n_hidden_units\n",
    "                torch.nn.Tanh(),   # 1st transfer function,\n",
    "                torch.nn.Linear(h, 1), # n_hidden_units to 1 output neuron\n",
    "                Squeeze() # remove the extra dimension\n",
    "                ) \n",
    "            \n",
    "            \n",
    "            #Train the model\n",
    "            net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                                loss_fn,\n",
    "                                                                X=X_train_ANN,\n",
    "                                                                y=y_train_ANN,\n",
    "                                                                n_replicates=n_replicates,\n",
    "                                                                max_iter=max_iter)\n",
    "            # Test model\n",
    "            y_test_est_ANN = net(X_val_ANN)\n",
    "            \n",
    "            # Determine errors and errors\n",
    "            se = (y_test_est_ANN.float()-y_val_ANN.float())**2 # squared error\n",
    "            Eval_ANN[k_2, h_index] = (sum(se).type(torch.float)/len(y_val)).data.numpy() #mean\n",
    "            h_index = h_index + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #Linear regression\n",
    "        \n",
    "        # Standardize the training and set set based on training set moments\n",
    "        mu_rlr = np.mean(X_train[:, 1:], 0)\n",
    "        sigma_rlr = np.std(X_train[:, 1:], 0)\n",
    "        X_train_rlr[:, 1:] = (X_train[:, 1:] - mu_rlr) / sigma_rlr\n",
    "        X_val_rlr[:, 1:] = (X_val[:, 1:] - mu_rlr) / sigma_rlr\n",
    "        \n",
    "        Xty = X_train_rlr.T @ y_train\n",
    "        XtX = X_train_rlr.T @ X_train_rlr\n",
    "        \n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "            lambdaI = lambdas[l] * np.eye(M)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,k_2,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            # Evaluate training and test performance\n",
    "            Eval_rlr[k_2,l] = np.power(y_val-X_val_rlr @ w[:,k_2,l].T,2).mean(axis=0)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "        k_2 = k_2 + 1\n",
    "        \n",
    "        \n",
    "           \n",
    "\n",
    "    #ANN    \n",
    "    h_index = 0\n",
    "    for h in h_values:  \n",
    "        Egen_ANN_temp[h_index] = np.dot(size_val, Eval_ANN[:, h_index])/size_par[k_1]\n",
    "        h_index = h_index + 1\n",
    "    h_opti = h_values[int(np.argmin(Egen_ANN_temp))]  #choice of parameter\n",
    "    optimal_ANN[k_1] = h_opti \n",
    "    \n",
    "    # Define the model\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "            torch.nn.Linear(M,h_opti), #M features to n_hidden_units\n",
    "            torch.nn.Tanh(),   # 1st transfer function,\n",
    "            torch.nn.Linear(h_opti, 1), # n_hidden_units to 1 output neuron\n",
    "            Squeeze() # remove the extra dimension\n",
    "            ) \n",
    "    #Train the model\n",
    "    net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                        loss_fn,\n",
    "                                                        X=X_par_ANN,\n",
    "                                                        y=y_par_ANN,\n",
    "                                                        n_replicates=n_replicates,\n",
    "                                                        max_iter=max_iter)\n",
    "    #Test model\n",
    "    y_test_est_ANN = net(X_test_ANN)\n",
    "    # Determine errors and errors\n",
    "    se = (y_test_est_ANN.float()-y_test_ANN.float())**2 # squared error\n",
    "    Etest_ANN[k_1] = (sum(se).type(torch.float)/len(y_test)).data.numpy() #mean \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Linear regression\n",
    "    for l in range(0,len(lambdas)):\n",
    "        Egen_rlr_temp[l] = np.dot(size_val, Eval_rlr[:, l])/size_par[k_1]\n",
    "    opt_lambda[k_1] = lambdas[np.argmin(Egen_rlr_temp)]\n",
    "    #we should obtain the same kfold than for the rest since the function used inside rlr_validate\n",
    "    #is the same\n",
    "    \n",
    "    mu[k_1, :] = np.mean(X_par[:, 1:], 0)\n",
    "    sigma[k_1, :] = np.std(X_par[:, 1:], 0)\n",
    "    \n",
    "    X_par_rlr[:, 1:] = (X_par[:, 1:] - mu[k_1, :] ) / sigma[k_1, :] \n",
    "    X_test_rlr[:, 1:] = (X_test[:, 1:] - mu[k_1, :] ) / sigma[k_1, :] \n",
    "    \n",
    "    Xty = X_par_rlr.T @ y_par\n",
    "    XtX = X_par_rlr.T @ X_par_rlr\n",
    "    \n",
    "    lambdaI = opt_lambda[k_1] * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k_1] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    Error_test_rlr[k_1] = np.square(y_test-X_test_rlr @ w_rlr[:,k_1]).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    \n",
    "    #baseline\n",
    "    Etest_nofeatures[k_1] = np.square(y_test-y_par.mean()).sum()/y_test.shape[0]\n",
    "    #DIVISER PAR TEST\n",
    "                  \n",
    "    k_1 = k_1 + 1\n",
    "    \n",
    "# %%\n",
    "\n",
    "#Final generalisation error baseline\n",
    "Egen_nofeatures = np.dot(size_test,  Etest_nofeatures)/N     \n",
    "#Final generalisation error ANN\n",
    "Egen_ANN  = np.dot(size_test, Etest_ANN)/N\n",
    "#Final generalisation error rlr\n",
    "Egen_rlr = np.dot(size_test, Error_test_rlr)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Tableau\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Visualizza i risultati\n",
    "\n",
    "# Crea un MultiIndex per le colonne\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    ('Outer fold', 'i'),\n",
    "    ('ANN', 'h<sub>i</sub><sup>*</sup>'),\n",
    "    ('ANN', 'E<sub>i</sub><sup>test</sup>'),\n",
    "    ('Linear Regression', '&lambda;<sub>i</sub><sup>*</sup>'),\n",
    "    ('Linear Regression', 'E<sub>i</sub><sup>test</sup>'),\n",
    "    ('Baseline', 'E<sub>i</sub><sup>test</sup>')\n",
    "])\n",
    "outer_fold_i = np.reshape(range(1, K1 + 1), (K1))\n",
    "opt_lambda = np.reshape(opt_lambda, (K1))\n",
    "Error_test_rlr = np.reshape(Error_test_rlr, (K1))\n",
    "Etest_nofeatures = np.reshape(Etest_nofeatures, (K1))\n",
    "\n",
    "# Convert lists to pandas Series before applying the format() function\n",
    "dff = pd.DataFrame(list(zip(pd.Series(outer_fold_i), pd.Series(optimal_ANN), pd.Series(Etest_ANN), pd.Series(opt_lambda), pd.Series(Error_test_rlr), pd.Series(Etest_nofeatures))), columns=columns)\n",
    "\n",
    "    \n",
    "# Apply the format() function\n",
    "df_styled = dff.style.set_properties(**{'text-align': 'center'}).format(\"{:.2f}\")\n",
    "\n",
    "# Aggiungi CSS personalizzato per allineare le intestazioni delle colonne\n",
    "styles = \"\"\"\n",
    "    <style>\n",
    "        th {\n",
    "            text-align: center;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "# Visualizza il DataFrame come HTML\n",
    "display(HTML(styles + df_styled.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Statistical comparison\n",
    "\n",
    "import numpy as np, scipy.stats as st\n",
    "\n",
    "K = 10\n",
    "CV = model_selection.KFold(K,shuffle=True)\n",
    "\n",
    "# store predictions.\n",
    "yhat = []\n",
    "y_true = []\n",
    "i=0\n",
    "for train_index, test_index in CV.split(X, y): \n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    X_train_rlr_stat = X[train_index,:]\n",
    "    X_test_rlr_stat = X[test_index,:]\n",
    "    X_train_ANN = torch.from_numpy(X_train).float()\n",
    "    y_train_ANN = torch.from_numpy(y_train).float()\n",
    "    X_test_ANN = torch.from_numpy(X_test).float()\n",
    "    y_test_ANN = torch.from_numpy(y_test).float()\n",
    "    \n",
    "\n",
    "    # Fit classifier and classify the test points (consider 1 to 40 neighbors)\n",
    "    dy = []\n",
    "    \n",
    "    # linear regression \n",
    "    # trova lambda_star come il parametro di controllo ottimale\n",
    "    index = np.argmin(Error_test_rlr)\n",
    "    lambda_star = opt_lambda[index]\n",
    "    mu_rlr = np.mean(X_train[:, 1:], 0)\n",
    "    sigma_rlr = np.std(X_train[:, 1:], 0)\n",
    "    X_train_rlr_stat[:, 1:] = (X_train[:, 1:] - mu_rlr) / sigma_rlr\n",
    "    X_test_rlr_stat[:, 1:] = (X_test[:, 1:] - mu_rlr) / sigma_rlr\n",
    "    \n",
    "    Xty = X_train_rlr_stat.T @ y_train\n",
    "    XtX = X_train_rlr_stat.T @ X_train_rlr_stat\n",
    "    \n",
    "    # Compute parameters for current value of lambda and current CV fold\n",
    "    # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "    lambdaI = lambda_star * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # remove bias regularization\n",
    "    w_rlr_stat = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    y_est = X_test_rlr_stat @ w_rlr_stat\n",
    "    dy.append( y_est )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ann\n",
    "    h_star = 500\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "            torch.nn.Linear(M, h_star), #M features to n_hidden_units\n",
    "            torch.nn.Tanh(),   # 1st transfer function,\n",
    "            torch.nn.Linear(h_star, 1), # n_hidden_units to 1 output neuron\n",
    "            Squeeze() # remove the extra dimension\n",
    "            ) \n",
    "    # Train the model\n",
    "    net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                      loss_fn,\n",
    "                                                      X=X_train_ANN,\n",
    "                                                      y=y_train_ANN,\n",
    "                                                      n_replicates=n_replicates,\n",
    "                                                      max_iter=max_iter)\n",
    "    # Test model\n",
    "    y_est = net(X_test_ANN).detach().numpy()\n",
    "    dy.append( y_est )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # baseline\n",
    "    y_est = y_train.mean()*np.ones(len(y_test)) \n",
    "    dy.append( y_est )\n",
    "\n",
    "\n",
    "    dy = np.stack(dy, axis=1)\n",
    "    yhat.append(dy)\n",
    "    y_true.append(y_test)\n",
    "    i+=1\n",
    "\n",
    "yhat = np.concatenate(yhat)\n",
    "y_true = np.concatenate(y_true)\n",
    "\n",
    "\n",
    "## Confidence interval test\n",
    "\n",
    "# %%\n",
    "# Significance level used for the statistical tests\n",
    "alpha = 0.05\n",
    "\n",
    "# First comparison\n",
    "zA = np.abs(y_true - yhat[:,0] ) ** 2\n",
    "zB = np.abs(y_true - yhat[:,1] ) ** 2\n",
    "\n",
    "z = zA - zB\n",
    "CI = st.t.interval(1-alpha, len(z)-1, loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Linear regresssion vs. ANN\")\n",
    "print(\"z = mean(Z_A-Z_B) estimator\", z.mean(), \" CI: \", CI, \"p-value\", p)\n",
    "print()\n",
    "\n",
    "# Second comparison\n",
    "zA = np.abs(y_true - yhat[:,1] ) ** 2\n",
    "zB = np.abs(y_true - yhat[:,2] ) ** 2\n",
    "\n",
    "z = zA - zB\n",
    "CI = st.t.interval(1-alpha, len(z)-1, loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "\n",
    "print()\n",
    "print(\"ANN vs. Baseline\")\n",
    "print(\"z = mean(Z_A-Z_B) estimator\", z.mean(), \" CI: \", CI, \"p-value\", p)\n",
    "print()\n",
    "\n",
    "# Third comparison\n",
    "zA = np.abs(y_true - yhat[:,0] ) ** 2\n",
    "zB = np.abs(y_true - yhat[:,2] ) ** 2\n",
    "\n",
    "z = zA - zB\n",
    "CI = st.t.interval(1-alpha, len(z)-1, loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "\n",
    "print()\n",
    "print(\"Linear  Regression vs. Baseline\")\n",
    "print(\"z = mean(Z_A-Z_B) estimator\", z.mean(), \" CI: \", CI, \"p-value\", p) \n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
