{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t23.199331\t0.0007131231\n",
      "\t\t2000\t14.869555\t0.00037825946\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t14.869555\t0.00037825946\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t16.791817\t0.00064272183\n",
      "\t\t2000\t11.570046\t0.00021714573\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t11.570046\t0.00021714573\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t20.782825\t0.0008152167\n",
      "\t\t2000\t13.323395\t0.0003171372\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t13.323395\t0.0003171372\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t16.339045\t0.00060106046\n",
      "\t\t2000\t11.343537\t0.00020391718\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t11.343537\t0.00020391718\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t23.298977\t0.00066200533\n",
      "\t\t2000\t16.753866\t0.0001453593\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t16.753866\t0.0001453593\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t21.611063\t0.0010020467\n",
      "\t\t2000\t12.950984\t0.00038173678\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t12.950984\t0.00038173678\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t15.845588\t0.000654089\n",
      "\t\t2000\t10.730753\t0.00022826265\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t10.730753\t0.00022826265\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t21.851313\t0.0007412202\n",
      "\t\t2000\t13.598739\t0.0003164649\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t13.598739\t0.0003164649\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t15.338419\t0.0007341267\n",
      "\t\t2000\t10.880609\t0.00014731624\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t10.880609\t0.00014731624\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t24.191202\t0.00052908337\n",
      "\t\t2000\t16.27892\t0.00019035979\n",
      "\t\tFinal loss:\n",
      "\t\t2000\t16.27892\t0.00019035979\n"
     ]
    }
   ],
   "source": [
    "# %% Import and standardize datas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.pyplot import (figure, title, boxplot, xticks, subplot, hist,\n",
    "                               xlabel, ylim, yticks, show, savefig)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('hour.csv')\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "# Removing useless attributes\n",
    "df = df.drop('dteday', axis=1)\n",
    "df = df.drop('instant', axis=1)\n",
    "df = df.drop('yr', axis=1)\n",
    "df = df.drop('atemp', axis=1)\n",
    "\n",
    "# Applying sqrt to \"cnt\" (to make it a continuous variable)\n",
    "df['cnt'] = np.sqrt(df['cnt'])\n",
    "\n",
    "# Removing deprecated attributes after the sqrt transformation (cnt = casual + registered)\n",
    "df = df.drop('casual', axis=1)\n",
    "df = df.drop('registered', axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Format data like exercises\n",
    "X = df.drop(columns=['cnt']).values\n",
    "N, M = X.shape\n",
    "y = df['cnt'].values\n",
    "attributeNames = df.columns.drop('cnt').tolist()\n",
    "\n",
    "# apply a feature transformation to your data matrix x such that each column has mean 0 and standard deviation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "df = pd.DataFrame(X, columns=attributeNames)\n",
    "\n",
    "# %% Define variables for double cross validation\n",
    "\n",
    "from matplotlib.pyplot import figure, plot, subplot, title, xlabel, ylabel, show, clim\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "from toolbox_02450 import feature_selector_lr, bmplot\n",
    "import numpy as np\n",
    "from toolbox_02450 import train_neural_net, draw_neural_net\n",
    "import torch\n",
    "from toolbox_02450 import rlr_validate\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K1 = 2\n",
    "K2 = 2\n",
    "CV1 = model_selection.KFold(n_splits=K1,shuffle=True)\n",
    "size_val = np.empty(K2)\n",
    "size_par = np.empty(K1)\n",
    "size_test = np.empty(K1)\n",
    "  \n",
    "#ANN\n",
    "h = 0\n",
    "h_values = [10, 20]\n",
    "h_number = len(h_values)\n",
    "n_replicates = 1        # number of networks trained in each k-fold\n",
    "max_iter = 2000\n",
    "loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "Eval_ANN = np.empty((K2,h_number))\n",
    "Egen_ANN_temp = np.empty((h_number))\n",
    "Etest_ANN = np.empty(K1)\n",
    "optimal_ANN = np.empty(K1) \n",
    "\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.squeeze(input)\n",
    "\n",
    "#baseline \n",
    "Error_val_nofeatures = np.empty((K2,1))\n",
    "Etest_nofeatures = np.empty((K1,1))\n",
    "\n",
    "#Linear regression\n",
    "lambdas = np.power(10.,range(-1,9))\n",
    "opt_lambda = np.empty((K1,1)) \n",
    "mu = np.empty((K1, M-1))\n",
    "sigma = np.empty((K1, M-1))\n",
    "w_rlr = np.empty((M,K1))\n",
    "w = np.empty((M,K2,len(lambdas)))\n",
    "Error_test_rlr = np.empty((K1,1))\n",
    "Eval_rlr = np.empty((K2,len(lambdas))) \n",
    "Egen_rlr_temp = np.empty((len(lambdas)))\n",
    "\n",
    " # %% Double crosss validation\n",
    " \n",
    "k_1 = 0\n",
    "for par_index, test_index in CV1.split(X):\n",
    "    \n",
    "    \n",
    "    X_par = X[par_index,:]\n",
    "    y_par = y[par_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]      \n",
    "    size_par[k_1] = len(par_index)\n",
    "    size_test[k_1] = len(test_index)\n",
    "    X_par_rlr = X[par_index,:]\n",
    "    X_test_rlr = X[test_index,:]\n",
    "    \n",
    "    X_test_ANN = torch.from_numpy(X_test).float()\n",
    "    y_test_ANN = torch.from_numpy(y_test).float()\n",
    "    X_par_ANN = torch.from_numpy(X_par).float()\n",
    "    y_par_ANN = torch.from_numpy(y_par).float()\n",
    "    \n",
    "    CV2 = model_selection.KFold(n_splits=K2,shuffle=True)\n",
    "    \n",
    "    k_2 = 0\n",
    "    for train_index, val_index in CV2.split(X_par):\n",
    "        # extract training and test set for current CV fold\n",
    "        X_train = X[train_index,:]\n",
    "        y_train = y[train_index]\n",
    "        X_val = X[val_index,:]\n",
    "        y_val = y[val_index]      \n",
    "        size_val[k_2] = len(val_index) \n",
    "        X_train_rlr = X[train_index,:]\n",
    "        X_val_rlr = X[val_index,:]\n",
    "        \n",
    "        X_train_ANN = torch.from_numpy(X_train).float()\n",
    "        y_train_ANN = torch.from_numpy(y_train).float()\n",
    "        X_val_ANN = torch.from_numpy(X_val).float()\n",
    "        y_val_ANN = torch.from_numpy(y_val).float()\n",
    "    \n",
    "    \n",
    "        #ANN\n",
    "        \n",
    "        \n",
    "        h_index = 0\n",
    "        for h in h_values:\n",
    "            # Define the model\n",
    "            model = lambda: torch.nn.Sequential(\n",
    "                torch.nn.Linear(M, h), #M features to n_hidden_units\n",
    "                torch.nn.Tanh(),   # 1st transfer function,\n",
    "                torch.nn.Linear(h, 1), # n_hidden_units to 1 output neuron\n",
    "                Squeeze() # remove the extra dimension\n",
    "                ) \n",
    "            \n",
    "            \n",
    "            #Train the model\n",
    "            net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                                loss_fn,\n",
    "                                                                X=X_train_ANN,\n",
    "                                                                y=y_train_ANN,\n",
    "                                                                n_replicates=n_replicates,\n",
    "                                                                max_iter=max_iter)\n",
    "            # Test model\n",
    "            y_test_est_ANN = net(X_val_ANN)\n",
    "            \n",
    "            # Determine errors and errors\n",
    "            se = (y_test_est_ANN.float()-y_val_ANN.float())**2 # squared error\n",
    "            Eval_ANN[k_2, h_index] = (sum(se).type(torch.float)/len(y_val)).data.numpy() #mean\n",
    "            h_index = h_index + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #Linear regression\n",
    "        \n",
    "        # Standardize the training and set set based on training set moments\n",
    "        mu_rlr = np.mean(X_train[:, 1:], 0)\n",
    "        sigma_rlr = np.std(X_train[:, 1:], 0)\n",
    "        X_train_rlr[:, 1:] = (X_train[:, 1:] - mu_rlr) / sigma_rlr\n",
    "        X_val_rlr[:, 1:] = (X_val[:, 1:] - mu_rlr) / sigma_rlr\n",
    "        \n",
    "        Xty = X_train_rlr.T @ y_train\n",
    "        XtX = X_train_rlr.T @ X_train_rlr\n",
    "        \n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "            lambdaI = lambdas[l] * np.eye(M)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,k_2,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            # Evaluate training and test performance\n",
    "            Eval_rlr[k_2,l] = np.power(y_val-X_val_rlr @ w[:,k_2,l].T,2).mean(axis=0)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "        k_2 = k_2 + 1\n",
    "        \n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #ANN    \n",
    "    h_index = 0\n",
    "    for h in h_values:  \n",
    "        Egen_ANN_temp[h_index] = np.dot(size_val, Eval_ANN[:, h_index])/size_par[k_1]\n",
    "        h_index = h_index + 1\n",
    "    h_opti = h_values[int(np.argmin(Egen_ANN_temp))]  #choice of parameter\n",
    "    optimal_ANN[k_1] = h_opti \n",
    "    \n",
    "    # Define the model\n",
    "    model = lambda: torch.nn.Sequential(\n",
    "            torch.nn.Linear(M,h_opti), #M features to n_hidden_units\n",
    "            torch.nn.Tanh(),   # 1st transfer function,\n",
    "            torch.nn.Linear(h_opti, 1), # n_hidden_units to 1 output neuron\n",
    "            Squeeze() # remove the extra dimension\n",
    "            ) \n",
    "    #Train the model\n",
    "    net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                        loss_fn,\n",
    "                                                        X=X_par_ANN,\n",
    "                                                        y=y_par_ANN,\n",
    "                                                        n_replicates=n_replicates,\n",
    "                                                        max_iter=max_iter)\n",
    "    #Test model\n",
    "    y_test_est_ANN = net(X_test_ANN)\n",
    "    # Determine errors and errors\n",
    "    se = (y_test_est_ANN.float()-y_test_ANN.float())**2 # squared error\n",
    "    Etest_ANN[k_1] = (sum(se).type(torch.float)/len(y_test)).data.numpy() #mean \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Linear regression\n",
    "    for l in range(0,len(lambdas)):\n",
    "        Egen_rlr_temp[l] = np.dot(size_val, Eval_rlr[:, l])/size_par[k_1]\n",
    "    opt_lambda[k_1] = lambdas[np.argmin(Egen_rlr_temp)]\n",
    "    #we should obtain the same kfold than for the rest since the function used inside rlr_validate\n",
    "    #is the same\n",
    "    \n",
    "    mu[k_1, :] = np.mean(X_par[:, 1:], 0)\n",
    "    sigma[k_1, :] = np.std(X_par[:, 1:], 0)\n",
    "    \n",
    "    X_par_rlr[:, 1:] = (X_par[:, 1:] - mu[k_1, :] ) / sigma[k_1, :] \n",
    "    X_test_rlr[:, 1:] = (X_test[:, 1:] - mu[k_1, :] ) / sigma[k_1, :] \n",
    "    \n",
    "    Xty = X_par_rlr.T @ y_par\n",
    "    XtX = X_par_rlr.T @ X_par_rlr\n",
    "    \n",
    "    lambdaI = opt_lambda[k_1] * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k_1] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "    Error_test_rlr[k_1] = np.square(y_test-X_test_rlr @ w_rlr[:,k_1]).sum(axis=0)/y_test.shape[0]\n",
    "    \n",
    "    \n",
    "    #baseline\n",
    "    Etest_nofeatures[k_1] = np.square(y_test-y_par.mean()).sum()/y_par.shape[0]\n",
    "                  \n",
    "    k_1 = k_1 + 1\n",
    "    \n",
    "# %%\n",
    "\n",
    "#Final generalisation error baseline\n",
    "Egen_nofeatures = np.dot(size_test,  Etest_nofeatures)/N     \n",
    "#Final generalisation error ANN\n",
    "Egen_ANN  = np.dot(size_test, Etest_ANN)/N\n",
    "#Final generalisation error rlr\n",
    "Egen_rlr = np.dot(size_test, Error_test_rlr)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        th {\n",
       "            text-align: center;\n",
       "        }\n",
       "    </style>\n",
       "<style type=\"text/css\">\n",
       "#T_10a2f_row0_col0, #T_10a2f_row0_col1, #T_10a2f_row0_col2, #T_10a2f_row0_col3, #T_10a2f_row0_col4, #T_10a2f_row0_col5, #T_10a2f_row1_col0, #T_10a2f_row1_col1, #T_10a2f_row1_col2, #T_10a2f_row1_col3, #T_10a2f_row1_col4, #T_10a2f_row1_col5 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_10a2f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_10a2f_level0_col0\" class=\"col_heading level0 col0\" >Outer fold</th>\n",
       "      <th id=\"T_10a2f_level0_col1\" class=\"col_heading level0 col1\" colspan=\"2\">ANN</th>\n",
       "      <th id=\"T_10a2f_level0_col3\" class=\"col_heading level0 col3\" colspan=\"2\">Linear Regression</th>\n",
       "      <th id=\"T_10a2f_level0_col5\" class=\"col_heading level0 col5\" >Baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_10a2f_level1_col0\" class=\"col_heading level1 col0\" >i</th>\n",
       "      <th id=\"T_10a2f_level1_col1\" class=\"col_heading level1 col1\" >h<sub>i</sub></th>\n",
       "      <th id=\"T_10a2f_level1_col2\" class=\"col_heading level1 col2\" >E<sub>i</sub><sup>test</sup></th>\n",
       "      <th id=\"T_10a2f_level1_col3\" class=\"col_heading level1 col3\" >&lambda;<sub>i</sub></th>\n",
       "      <th id=\"T_10a2f_level1_col4\" class=\"col_heading level1 col4\" >E<sub>i</sub><sup>test</sup></th>\n",
       "      <th id=\"T_10a2f_level1_col5\" class=\"col_heading level1 col5\" >E<sub>i</sub><sup>test</sup></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_10a2f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_10a2f_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
       "      <td id=\"T_10a2f_row0_col1\" class=\"data row0 col1\" >20.00</td>\n",
       "      <td id=\"T_10a2f_row0_col2\" class=\"data row0 col2\" >16.70</td>\n",
       "      <td id=\"T_10a2f_row0_col3\" class=\"data row0 col3\" >100.00</td>\n",
       "      <td id=\"T_10a2f_row0_col4\" class=\"data row0 col4\" >171.63</td>\n",
       "      <td id=\"T_10a2f_row0_col5\" class=\"data row0 col5\" >45.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_10a2f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_10a2f_row1_col0\" class=\"data row1 col0\" >2.00</td>\n",
       "      <td id=\"T_10a2f_row1_col1\" class=\"data row1 col1\" >20.00</td>\n",
       "      <td id=\"T_10a2f_row1_col2\" class=\"data row1 col2\" >16.65</td>\n",
       "      <td id=\"T_10a2f_row1_col3\" class=\"data row1 col3\" >10.00</td>\n",
       "      <td id=\"T_10a2f_row1_col4\" class=\"data row1 col4\" >168.20</td>\n",
       "      <td id=\"T_10a2f_row1_col5\" class=\"data row1 col5\" >44.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Tableau\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Visualizza i risultati\n",
    "\n",
    "# Crea un MultiIndex per le colonne\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    ('Outer fold', 'i'),\n",
    "    ('ANN', 'h<sub>i</sub>'),\n",
    "    ('ANN', 'E<sub>i</sub><sup>test</sup>'),\n",
    "    ('Linear Regression', '&lambda;<sub>i</sub>'),\n",
    "    ('Linear Regression', 'E<sub>i</sub><sup>test</sup>'),\n",
    "    ('Baseline', 'E<sub>i</sub><sup>test</sup>')\n",
    "])\n",
    "outer_fold_i = np.reshape(range(1, K1 + 1), (K1))\n",
    "opt_lambda = np.reshape(opt_lambda, (K1))\n",
    "Error_test_rlr = np.reshape(Error_test_rlr, (K1))\n",
    "Etest_nofeatures = np.reshape(Etest_nofeatures, (K1))\n",
    "\n",
    "# Convert lists to pandas Series before applying the format() function\n",
    "dff = pd.DataFrame(list(zip(pd.Series(outer_fold_i), pd.Series(optimal_ANN), pd.Series(Etest_ANN), pd.Series(opt_lambda), pd.Series(Error_test_rlr), pd.Series(Etest_nofeatures))), columns=columns)\n",
    "\n",
    "    \n",
    "# Apply the format() function\n",
    "df_styled = dff.style.set_properties(**{'text-align': 'center'}).format(\"{:.2f}\")\n",
    "\n",
    "# Aggiungi CSS personalizzato per allineare le intestazioni delle colonne\n",
    "styles = \"\"\"\n",
    "    <style>\n",
    "        th {\n",
    "            text-align: center;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "# Visualizza il DataFrame come HTML\n",
    "display(HTML(styles + df_styled.to_html()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
